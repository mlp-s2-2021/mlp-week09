{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Python - Workshop 9\n",
    "\n",
    "In this week's workshop we will be returning to the NYC Parking Ticket data and exploring how to score and evaluate multiclass classification models as well as trying several addition modeling approaches for this type of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "\n",
    "\n",
    "## 1.1 Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we will load the core libraries we will be using for this workshop and setting some sensible defaults for our plot size and resolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "\n",
    "# sklearn modules\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "import sklearn.ensemble \n",
    "import sklearn.neighbors\n",
    "import sklearn.preprocessing\n",
    "\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Helper Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundaries(bounds, x='lon', y='lat', group='precinct', n=5):\n",
    "    \"\"\" Draws boundary lines for a series of groups polygons in a dataframe\n",
    "    \"\"\"\n",
    "    sns.lineplot(x=x, y=y, hue=group, data=bounds,\n",
    "                 sort=False, palette=['k']*n, legend=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_labels(res, pred = 'pred_label', truth = 'precinct'):\n",
    "    \"\"\" Plots the predicted labels and true labels from a common data frame\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(5,8))\n",
    "    \n",
    "    ax = sns.scatterplot(\n",
    "        x='lon', y='lat', hue=pred, palette=precinct_pal, data=res\n",
    "    )\n",
    "    plot_boundaries(manh_bounds)\n",
    "    \n",
    "    acc = sklearn.metrics.accuracy_score(\n",
    "        res[truth], res[pred]\n",
    "    )\n",
    "    \n",
    "    ax.set_title(\"Predicted Labels (Accuracy {:.3f})\".format(acc))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data\n",
    "\n",
    "As described in the Week 7 workshop, these data comes from New York City's [Open Data project](https://opendata.cityofnewyork.us/). We have simplified the data somewhat and restricted the data to just include the five precincts (1st, 5th, 6th, 7th, and 9th) in the southern end of Manhattan. The following data files have been provided:\n",
    "\n",
    "* `manh_tickets.csv` - Geocoded parking tickets from the 5 southern most precincts in Manhattan\n",
    "* `manh_test.csv` - Points randomly sampled within the true boundaries of these precincts\n",
    "* `manh_bounds.csv` - boundaries of these precincts\n",
    "\n",
    "As before, our goal is to use these parking tickets to develop a model which correctly predicts the boundaries of the police precincts in Manhattan based only on the locations where parking tickets have been issued. We will read in all the data sets using pandas,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manh_tickets = pd.read_csv(\"manh_tickets.csv\")\n",
    "manh_test    = pd.read_csv(\"manh_test.csv\")\n",
    "manh_bounds  = pd.read_csv(\"manh_bounds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and create our basic response vector and model matrix,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = manh_tickets[['lon','lat']]\n",
    "y = manh_tickets.precinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create labels and a color palette which will be used for across subsequent plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precincts = ['Precinct01', 'Precinct05', 'Precinct06', 'Precinct07', 'Precinct09']\n",
    "precincts_short = ['P01', 'P05', 'P06', 'P07', 'P09']\n",
    "precincts_pred = [\"pred_\" + p for p in precincts_short]\n",
    "\n",
    "# Create a color palette for precincts based on the cols25 palette from R's pals package\n",
    "precinct_pal = dict(\n",
    "    zip(precincts,\n",
    "        [(0.1215686, 0.47058824, 0.7843137), (1.0000000, 0.00000000, 0.0000000),\n",
    "         (0.2000000, 0.62745098, 0.1725490), (0.4156863, 0.20000000, 0.7607843),\n",
    "         (1.0000000, 0.49803922, 0.0000000) ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our new palette we can plot the original parking ticket data and add the precinct boundaries using `plot_boundaries` to make everything more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-62957fd32341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m sns.scatterplot(\n\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'precinct'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecinct_pal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanh_tickets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ).set_title(\"Parking Tickets\")\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(6,10))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x='lon', y='lat', hue='precinct', palette=precinct_pal, data=manh_tickets\n",
    ").set_title(\"Parking Tickets\")\n",
    "plot_boundaries(manh_bounds)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# 1.4. Review - Multiclass logistic (multinomial) regression \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the Week 7 workshop we had fit a Logistic Regression model using `multi_class=multinomial` in order to obtain a probabilistically consistent predictive model, e.g. for any given prediction the probabilities of each class sum to one. We will begin by reconstructing this same model and using it as a point of comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_mn = make_pipeline(\n",
    "    sklearn.preprocessing.StandardScaler(),\n",
    "    sklearn.linear_model.LogisticRegression(penalty='none', multi_class='multinomial')\n",
    ").fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is then used to predict both the class label for test location as well as the predicted probability for each precinct for all test locations, these results are then stored in the `res_mn` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mn = pd.concat(\n",
    "    [ manh_test,\n",
    "      pd.Series(\n",
    "          data = m_mn.predict(manh_test[['lon','lat']]),\n",
    "          name = \"pred_label\"\n",
    "      ),\n",
    "      pd.DataFrame(\n",
    "          data = m_mn.predict_proba(manh_test[['lon','lat']]),\n",
    "          columns = precincts_pred\n",
    "      )\n",
    "    ], \n",
    "    axis = 1\n",
    ")\n",
    "\n",
    "res_mn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 1\n",
    "\n",
    "Pick at least three random test locactions and using `res_mn` verify that the predicted label for each point corresponds to the class with the largest predicted probability and that the predicted probabilities are consistent (e.g. they add up to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `plot_pred_labels` function, defined witht the helper functions above, to visualize the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_labels(res_mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Additional scoring methods / tools for multiclass models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Confusion Matrix\n",
    "\n",
    "As we saw in week 7, perhaps the most straight forward approach to assess a model is to use the label predictions directly to construct a confusion matrix which places the true labels along the rows and the predicted labels along the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(res_mn.precinct, res_mn.pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 4\n",
    "\n",
    "Explain what information is given by the value 27 given in the 2nd row, 5th column of this confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To expand on this idea, and to help scale this approach to cases when there are many classes sklearn provides a convenient plotting function for visualizing the confusion matrix which provides methods for normalizing by the rows (\"true\"), columns (\"pred\"), or all values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "normalize=[None, 'true', 'pred', 'all']\n",
    "\n",
    "for i in range(len(normalize)):\n",
    "    ax = fig.add_subplot(221+i)\n",
    "    \n",
    "    if (normalize[i] is None):\n",
    "        ax.set_title(\"normalize = None\")\n",
    "    else:\n",
    "        ax.set_title(\"normalize = \\\"\" + normalize[i] + \"\\\"\")\n",
    "    \n",
    "    sklearn.metrics.plot_confusion_matrix(\n",
    "        m_mn, X, y, include_values=False, ax=ax,\n",
    "        normalize = normalize[i],\n",
    "        display_labels = precincts_short,\n",
    "        cmap = plt.cm.Blues\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 2\n",
    "\n",
    "Which normalization method appears to be is most useful for determining False Positives, what about False Negatives? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2 One-vs-rest ROC curves\n",
    "\n",
    "Previously in the case of logistic regression (binary classification) we saw how this enabled us to consider all possible decision threshold values to generate a receiver operating characteristic (ROC) curve showing the trade off between possible true positive and false positive rates.\n",
    "\n",
    "In the case of a classification model that produces probabilistic predictions we can extend this idea to the multiclass case but it is not without some significant drawbacks. Specifically, the concept of a decision threshold does not make sense in the multiclass setting since we need to make a choice among $k$ classes. However, what we can do is consider a one-vs-rest approach where we take each class as the positive case and all other classes as the negative. In this way we can construct $k$ different ROC curves using our original predicted probabilities. The function below implements this considering each class separately and calculating a unique ROC and AUC for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ovr_roc_plot(y_true, y_pred):\n",
    "    \"\"\" Draw ROC curves using one-vs-rest approach\n",
    "    \"\"\"\n",
    "    \n",
    "    classes = y_true.unique()\n",
    "    n_classes = len(y_true.unique())\n",
    "    \n",
    "    # Convert from n x 1 categorical matrix to n x k binary matrix\n",
    "    y_true = pd.get_dummies(y_true).to_numpy()\n",
    "    \n",
    "    y_pred = y_pred.to_numpy()\n",
    "    \n",
    "    # Sanity Check\n",
    "    if y_true.shape[1] != y_pred.shape[1]:\n",
    "        raise ValueError(\"Truth and prediction dimensions do not match.\")\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    rocs = dict() \n",
    "    aucs = dict() \n",
    "    for name, i in zip(classes, range(n_classes)):\n",
    "        aucs[i] = pd.DataFrame({ \n",
    "            'precinct': [name],\n",
    "            'auc': [sklearn.metrics.roc_auc_score(y_true[:, i], y_pred[:, i])]\n",
    "        })\n",
    "        \n",
    "        rocs[i] = pd.DataFrame(\n",
    "            data = np.c_[sklearn.metrics.roc_curve(y_true[:, i], y_pred[:, i])],\n",
    "            columns = ('fpr', 'tpr', 'threshold')\n",
    "        ).assign(\n",
    "            precinct = name\n",
    "        )\n",
    "    \n",
    "    # Bind rows to create a single data frame for each\n",
    "    roc = pd.concat(rocs)\n",
    "    auc = pd.concat(aucs)\n",
    "    \n",
    "    # Create plot\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    sns.lineplot(x='fpr', y='tpr', hue='precinct', data=roc, ci=None, palette=precinct_pal)\n",
    "\n",
    "    plt.plot([0,1],[0,1], 'k--', alpha=0.5) # 0-1 line \n",
    "    plt.title(\"ROC (one-vs-rest) curves\")\n",
    "         \n",
    "    L = plt.legend()\n",
    "    for precinct, auc_val, i in zip(auc.precinct, auc.auc, range(n_classes)):\n",
    "        L.get_texts()[i].set_text(\"{} (auc {:.3f})\".format(precinct, auc_val))\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    # Return the AUCs as a Data Frame\n",
    "    return(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this function to evaluate our class probability predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_roc_plot(res_mn.precinct, res_mn[precincts_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 3\n",
    "\n",
    "Does the ordering of the AUC values agree with your intuition about the models performance for the different precincts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 4\n",
    "\n",
    "We have already established that this is reasonable for predicting most of these police precincts (accuracy of 0.895), however the AUCs reported for our model look very very good (all are >0.99). Can you explain this discrepancy? \n",
    "\n",
    "*Hint* - look at the definition of TPR and FPR and then think about what happens when we use the one-vs-rest approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Other multiclass classification models\n",
    "\n",
    "For this section we will explore a number approaches to fitting and predict multiclass classification models. For the sake of uniformity we will assess the models in the same way:\n",
    "\n",
    "* Plot all of the predicted labels and the true labels for a visual comparison and\n",
    "\n",
    "* Show the `classification_report` result for the predicted labels.\n",
    "\n",
    "To avoid repeated code we will use the following helper function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test_assess(model, inc_report = True, inc_proba = False):\n",
    "    # Use the model to predict test labels\n",
    "    res = pd.concat(\n",
    "        [ manh_test,\n",
    "          pd.Series(\n",
    "              data = model.predict(manh_test[['lon','lat']]),\n",
    "              name = \"pred_label\"\n",
    "          )\n",
    "        ], \n",
    "        axis = 1\n",
    "    )\n",
    "        \n",
    "    if (inc_proba):\n",
    "        res = pd.concat(\n",
    "            [ res, \n",
    "              pd.DataFrame(\n",
    "                  data = model.predict_proba(manh_test[['lon','lat']]),\n",
    "                  columns = precincts_pred\n",
    "              )\n",
    "            ], \n",
    "            axis = 1\n",
    "        )\n",
    "    \n",
    "    # Plot labels\n",
    "    plot_pred_labels(res)\n",
    "    \n",
    "    if inc_report: # Print report\n",
    "        print(\n",
    "            sklearn.metrics.classification_report(\n",
    "                res.precinct, res.pred_label,\n",
    "                zero_division = 0\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 `SVC`\n",
    "\n",
    "Standard Support Vector Classifier models are able to handle multiclass outcome vectors by fitting all of the one-to-one SVC models for each pair of classes. This therefore involves fitting ${k \\choose 2} = k(k-1)/2$ SVC models which can be slow. The labels are predicted based on using all ${k \\choose 2}$ generate binary predictions which are then tabulated and the label with the most votes is choosen. See [here](https://scikit-learn.org/stable/modules/svm.html#svm-classification) for more details.\n",
    "\n",
    "Below we define a function for fitting and assess this model using different kernel and penalty values. As SVMs can be sensitive to scale of the features we include a pipeline thats scales the latitude and logitude values before fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svc(kernel, C):\n",
    "    m_svc = make_pipeline(\n",
    "        sklearn.preprocessing.StandardScaler(),\n",
    "        sklearn.svm.SVC(C=C, kernel=kernel)\n",
    "    ).fit(X,y)\n",
    "    \n",
    "    model_test_assess(m_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 5\n",
    "\n",
    "Using this function try different values of `kernel` and `C`, what seems to produce the best model? Explain why you think this model is performing better than the alternatives.\n",
    "\n",
    "*Hint* - recommended kernels to try include `rbd`, `poly`, and `linear`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_svc(kernel = \"rbf\", C=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 6\n",
    "\n",
    "Comment out the line of code that includes the `StandardScaler` in the pipeline below. What happens to the models predictive performance? Try adjusting `C` and or `kernel` to see if you can improve things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_svc2 = make_pipeline(\n",
    "    sklearn.preprocessing.StandardScaler(),\n",
    "    sklearn.svm.SVC(C=1, kernel=\"rbf\")\n",
    ").fit(X,y)\n",
    "\n",
    "model_test_assess(m_svc2, inc_report=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.1.2 `LinearSVC`\n",
    "\n",
    "An alternative to the one-vs-one behavior of `SVC` is to instead fit a one-vs-rest model, in sklearn this is only supported by the `LinearSVC` classifier model. This modeling approach needs to only fit $k$ SVM models, and is therefore usually much faster for large $k$. However, due to implementation details of the underlying fitting library it does not support non-linear kernels. As with other SVM models it is important to scale our data before fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lsvc(C):\n",
    "    m_lsvc = make_pipeline(\n",
    "        sklearn.preprocessing.StandardScaler(),\n",
    "        sklearn.svm.LinearSVC(C=C, max_iter=5000)\n",
    "    ).fit(X,y)\n",
    "    \n",
    "    model_test_assess(m_lsvc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 7\n",
    "\n",
    "Using this function try different values of `C` to tune the model, how does its performance compare to the `SVC` model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_lsvc(C=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Tree based methods\n",
    "\n",
    "### 3.2.1 `DecisionTreeClassifier`\n",
    "\n",
    "This model fits a decision tree model that attempts to classify observations by constructing a binary decision tree on the features provided. In the case of binary and multiclass classification the predicted label is based on the most common label within the terminal node. For fitting this model for these data we will solely focus on the use of the `max_depth` parameter which determines the number of branchs within the tree. Keep in mind that each additional layer added to the tree potentially increases the number of nodes by a factor of 2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_dt(max_depth):\n",
    "    m_dt = sklearn.tree.DecisionTreeClassifier(\n",
    "        max_depth = max_depth\n",
    "    ).fit(X,y)\n",
    "    \n",
    "    model_test_assess(m_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code chunk to explore the effect of different max depths on the tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_dt(max_depth = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 8\n",
    "\n",
    "Using `max_depth=1` how many different classes are reflected in the predictions? Using `max_depth=2`? Based on this and given there are 5 classes we are attempting to classify, what is the minimum depth of tree should we be using?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 9\n",
    "\n",
    "Examine the boundaries being created by the model (particularly evident with small `max_depth` values), what shape do they have? Is the potentially problematic for the task at hand?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "The issue we've just seem is similar to the issue we saw with the original single precinct logistic regression model from Week 7, and we can somewhat address it in the same way by introducing an interaction feature between `lat` and `lon` before fitting our model by adding a `PolynomialFeature` step before fitting the tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_dt_int(max_depth):\n",
    "    m_dt_int = make_pipeline(\n",
    "        sklearn.preprocessing.PolynomialFeatures(\n",
    "            degree=2, interaction_only=True, include_bias=False\n",
    "        ),\n",
    "        sklearn.tree.DecisionTreeClassifier(\n",
    "            max_depth = max_depth\n",
    "        )\n",
    "    ).fit(X,y)\n",
    "    \n",
    "    model_test_assess(m_dt_int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_dt_int(max_depth = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 10\n",
    "\n",
    "What has changed about the boundaries being created by the model (particularly evident with small `max_depth` values), what shape do they have? Does this improve model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 11\n",
    "\n",
    "For either model, for what values of `max_depth` do you start seeing clear evidence of overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.2.2 `RandomForestClassifier`\n",
    "\n",
    "As discussed in lecture, this class of model is an extension of the decision tree frame work whereby a number decision tree models are fit to random sub-sample (i.e. bootstrap sample) of the features. When making predictions each tree predicts a label and the most common label across all of the trees is used as the final prediction. For now we will just examine the effect of `n_estimators` which corresponds to the number of trees and `max_depth` which is the maximum depth of the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rf(n_estimators, max_depth):\n",
    "    m_rf = sklearn.ensemble.RandomForestClassifier(\n",
    "        n_estimators=n_estimators, max_depth=max_depth\n",
    "    ).fit(X,y)\n",
    "    \n",
    "    model_test_assess(m_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code chunk to explore the effect of different values of `max_depth` and `n_estimators` on the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_rf(n_estimators = 5, max_depth = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 12\n",
    "\n",
    "Which combination of `n_estimators` and `max_depth` seems to produce the best model performance? How does it compare to the other models considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 13\n",
    "\n",
    "For a given `max_depth` (e.g. 4) compare the performance of a single decision tree model to random forrest models with different values of `n_estimators`, how do they compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 14\n",
    "\n",
    "Do you think the inclusion of an interaction feature would help or hurt this model's performance? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Comparisons\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &diams; Exercise 15\n",
    "\n",
    "You have now been given the opportunity to experiment with a number of different multiclass classification models. Based on your experience with this data set which model do you think is best for this particular classification task? Justify your answer.\n",
    "\n",
    "You answer should contain some discussion of the potential for overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00125-d4045e58-53c0-4b7f-8aa5-d6248ee21245",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Competing the worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the preceeding exercises. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF and turn it in on gradescope under the `mlp-week09` assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00126-f94a8e2e-6864-478b-9b51-1ca96400115d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10531,
    "execution_start": 1612353465438,
    "output_cleared": true,
    "source_hash": "950a0210",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp-week09.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
